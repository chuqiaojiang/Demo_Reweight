{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of distribution reweighting\n",
    "\n",
    "    xgboost is used to train the data and mc sample.\n",
    "    data is shuffled before divided into train test and validation part\n",
    "    root_numpy and pandas are used to hadle root files.\n",
    "    sklearn is partly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "figsize(16, 8)\n",
    "\n",
    "from __future__ import division\n",
    "import xgboost as xgb \n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import root_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist_settings = {'bins': 100, 'normed': True, 'alpha': 0.5}\n",
    "#column_name=['Px_p','Py_p','Pz_p','E_p','Px_pi','Py_pi','Pz_pi','E_pi','M_ppi', 'M_pn','M_npi']\n",
    "#column_name=columns\n",
    "def draw_distributions(original, target, new_original_weights, fig_name):\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    for id, column in enumerate(columns_draw, 1):\n",
    "        xlim = numpy.percentile(numpy.hstack([target[column]]), [0.01, 99.99]) #percentile is to calculate the p% number of a givening array\n",
    "        ax=plt.subplot(5, 5, id) \n",
    "        hist(target[column], range=xlim, **hist_settings)\n",
    "        hist(original[column], weights=new_original_weights, range=xlim, **hist_settings)  #hist is to plot the histogram (matplotlib)\n",
    "        \n",
    "        plt.title(columns[id-1],fontsize=20)\n",
    "        tight_layout() #distance between different figs\n",
    "        plt.xticks(fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n",
    "        # 设置坐标标签字体大小\n",
    "        #ax.set_xlabel('', fontsize=20)\n",
    "        #ax.set_ylabel('', fontsize=20)\n",
    "        # 设置图例字体大小\n",
    "        #ax.legend('', fontsize=20)    \n",
    "    savefig(fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-581d13715294>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-581d13715294>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    phsp_10w = pd.DataFrame(phsp_10w)\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#columns = ['Px_g','Py_g','Pz_g','E_g','Px_eta','Py_eta','Pz_eta','E_eta','Px_pi01','Py_pi01','Pz_pi01','E_pi01','Px_pi02','Py_pi02',\n",
    "#           'Pz_pi02','E_pi02','Metapi01','Metapi02','Mpi01pi02','Metapi01pi02',\n",
    "#           'Mgeta','Mgpi01','Mgpi02','Mgetapi01','Mgetapi02']\n",
    "columns = ['ct_g','ct_eta','Metapi01','Metapi02','Mpi01pi02','Mgeta','Mgpi01']\n",
    "columns_draw = ['Px_g','Py_g','Pz_g','E_g','Px_eta','Py_eta','Pz_eta','E_eta','Px_pi01','Py_pi01','Pz_pi01','E_pi01','Px_pi02','Py_pi02',\n",
    "           'Pz_pi02','E_pi02','Metapi01','Metapi02','Mpi01pi02','Metapi01pi02',\n",
    "           'Mgeta','Mgpi01','Mgpi02','Mgetapi01','Mgetapi02']\n",
    "#columns = ['p_px','p_py','p_pz','p_e','pi_px','pi_py','pi_pz','pi_e','ppi_mass', 'pn_mass','npi_mass']\n",
    "phsp_ori = root_numpy.root2array('/home/xiongxa/ML_reweighting/DATA/EtaPi0Pi0/Final/MC_cut_final.root', treename='final',branches=columns)\n",
    "phsp_10w = root_numpy.root2array('/home/xiongxa/ML_reweighting/DATA/EtaPi0Pi0/Final/MC_train20w_final.root', treename='final',branches=columns)\n",
    "data_ori = root_numpy.root2array('/home/xiongxa/ML_reweighting/DATA/EtaPi0Pi0/Final/DIY_after_final.root',treename='final', branches=columns)\n",
    "mctruth_ori = root_numpy.root2array('/home/xiongxa/ML_reweighting/DATA/EtaPi0Pi0/Final/MCtruth_final.root', treename='final',branches=columns)\n",
    "DIY_truth_all = root_numpy.root2array('/home/xiongxa/ML_reweighting/DATA/EtaPi0Pi0/Final/DIY_truth_all_final.root', treename='final',branches=columns)\n",
    "\n",
    "data_only_X=pd.DataFrame(data_ori)\n",
    "phsp_only_X=pd.DataFrame(phsp_ori)\n",
    "phsp_10w_X=pd.DataFrame(phsp_10w)\n",
    "mctruth_only_X=pd.DataFrame(mctruth_ori)\n",
    "DIY_truth_all_X=pd.DataFrame(DIY_truth_all)\n",
    "\n",
    "data_only_Y=numpy.ones(len(data_only_X))\n",
    "phsp_only_Y=numpy.zeros(len(phsp_only_X))\n",
    "phsp_10w_Y=numpy.zeros(len(phsp_10w_X))\n",
    "mctruth_only_Y=numpy.zeros(len(mctruth_only_X))\n",
    "DIY_truth_all_Y=numpy.zeros(len(DIY_truth_all_X)\n",
    "\n",
    "phsp = pd.DataFrame(phsp_ori)\n",
    "phsp_10w = pd.DataFrame(phsp_10w)\n",
    "data = pd.DataFrame(data_ori)\n",
    "original_weights = numpy.ones(len(phsp_ori))\n",
    "#for MC truth information\n",
    "\n",
    "mctruth = pd.DataFrame(mctruth_ori)\n",
    "mctruth_weights = numpy.ones(len(mctruth_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phsp_10w_a=np.array(phsp_10w)\n",
    "data_a=np.array(data)\n",
    "phsp_only_X=np.array(phsp)\n",
    "mctruth_only_X=np.array(mctruth)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_train, data_test = train_test_split(data_a,test_size=0)\n",
    "phsp_train, phsp_test1 = train_test_split(phsp_10w,train_size=data_train.shape[0]) #get the same number of mc with data\n",
    "\n",
    "data_all = numpy.concatenate([data_train, phsp_train])\n",
    "labels_all = numpy.array([1] * len(data_train) + [0] * len(phsp_train))\n",
    "print data_all,data_all.shape\n",
    "print labels_all,labels_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "train_X, test_X,train_Y, test_Y= train_test_split(data_all,labels_all,test_size=0.3)\n",
    "\n",
    "print train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(train_X, label=train_Y)\n",
    "xg_test = xgb.DMatrix(test_X, label=test_Y)\n",
    "#xg_val = xgb.DMatrix(val_X, label=val_Y)\n",
    "xg_data_only = xgb.DMatrix(data_only_X, label=data_only_Y)\n",
    "xg_phsp_only = xgb.DMatrix(phsp_only_X, label=phsp_only_Y)\n",
    "xg_mctruth_only = xgb.DMatrix(mctruth_only_X, label=mctruth_only_Y)\n",
    "xg_DIY_truth_all = xgb.DMatrix(DIY_truth_all_X, label=DIY_truth_all_Y)\n",
    "##参数\n",
    "'''\n",
    "params={\n",
    "'booster':'gbtree',\n",
    "'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.\n",
    "#'nthread':7,# cpu 线程数 默认最大\n",
    "'eta': 0.007, # 如同学习率\n",
    "'min_child_weight':3, \n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "'max_depth':6, # 构建树的深度，越大越容易过拟合\n",
    "'gamma':0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "'subsample':0.7, # 随机采样训练样本\n",
    "'colsample_bytree':0.7, # 生成树时进行的列采样 \n",
    "'lambda':2,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "#'alpha':0, # L1 正则项参数\n",
    "#'scale_pos_weight':1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。\n",
    "#'objective': 'multi:softmax', #多分类的问题\n",
    "'num_class':2, # 类别数，多分类与 multisoftmax 并用\n",
    "'seed':1000, #随机种子\n",
    "#'eval_metric': 'auc'\n",
    "}\n",
    "'''\n",
    "\n",
    "# setup parameters for xgboost\n",
    "params = {}\n",
    "# use softmax multi-class classification\n",
    "params['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 7\n",
    "params['silent'] = 1\n",
    "params['nthread'] = 4\n",
    "params['num_class'] = 2\n",
    "\n",
    "# do the same thing again, but output probabilities\n",
    "params['objective'] = 'multi:softprob'\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 200 # 迭代次数\n",
    "bst = xgb.train(params, xg_train, num_round, watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 2)\n",
    "print pred_prob\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\n",
    "print pred_label[0:20], test_Y[0:20] ,error_rate\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pr_test=np.array(bst.predict(xg_test).reshape(test_Y.shape[0], 2))\n",
    "#pr_val=np.array(bst.predict(xg_val).reshape(val_Y.shape[0], 2))\n",
    "pr_phsp=np.array(bst.predict(xg_phsp_only).reshape(phsp_only_Y.shape[0], 2))\n",
    "pr_mctruth=np.array(bst.predict(xg_mctruth_only).reshape(mctruth_only_Y.shape[0], 2))\n",
    "\n",
    "weight_test=pr_test[:,1]/pr_test[:,0]\n",
    "#weight_val=pr_val[:,1]/pr_val[:,0]\n",
    "weight_phsp=pr_phsp[:,1]/pr_phsp[:,0]\n",
    "weight_mctruth=pr_mctruth[:,1]/pr_mctruth[:,0]\n",
    "\n",
    "print 'The efficiency is = ',weight_phsp.sum()/weight_mctruth.sum()\n",
    "\n",
    "draw_distributions(phsp_ori, data_ori,weight_phsp,'phsp_target_xgboost.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,GridSearchCV\n",
    "Error=[]\n",
    "Effi=[]\n",
    "randn=np.random.RandomState(22222)\n",
    "kf=KFold(n_splits=10,shuffle=True,random_state=randn)\n",
    "for train_index, test_index in kf.split(train_X):\n",
    "    xgk_train = xgb.DMatrix(train_X[train_index], label=train_Y[train_index])\n",
    "    xgk_test = xgb.DMatrix(train_X[test_index], label=train_Y[test_index])\n",
    "    \n",
    "    print train_X[train_index],train_Y[train_index]\n",
    "    # setup parameters for xgboost\n",
    "    params = {}\n",
    "    # use softmax multi-class classification\n",
    "    params['objective'] = 'multi:softmax'\n",
    "    # scale weight of positive examples\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = 6\n",
    "    params['silent'] = 1\n",
    "    params['nthread'] = 4\n",
    "    params['num_class'] = 2\n",
    "\n",
    "    # do the same thing again, but output probabilities\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    watchlist = [(xgk_train, 'train'), (xgk_test, 'test')]\n",
    "    num_round = 200 # 迭代次数\n",
    "    bst = xgb.train(params, xgk_train, num_round, watchlist)\n",
    "    \n",
    "    pred_prob = bst.predict(xgk_test).reshape(train_Y[test_index].shape[0], 2)\n",
    "    pred_label = np.argmax(pred_prob, axis=1)\n",
    "    error_rate = np.sum(pred_label != train_Y[test_index]) / train_Y[test_index].shape[0]\n",
    "    \n",
    "    pr_test=np.array(bst.predict(xg_test).reshape(test_Y.shape[0], 2))\n",
    "    pr_phsp=np.array(bst.predict(xg_phsp_only).reshape(phsp_only_Y.shape[0], 2))\n",
    "    pr_mctruth=np.array(bst.predict(xg_mctruth_only).reshape(mctruth_only_Y.shape[0], 2))\n",
    "    \n",
    "    weight_test=pr_test[:,1]/pr_test[:,0]\n",
    "    weight_phsp=pr_phsp[:,1]/pr_phsp[:,0]\n",
    "    weight_mctruth=pr_mctruth[:,1]/pr_mctruth[:,0]\n",
    "    \n",
    "    Error.append(error_rate);\n",
    "    Effi.append(weight_phsp.sum()/weight_mctruth.sum());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sum(Error)/10, sum(Effi)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print the importance score for each viriable\n",
    "column_name=columns\n",
    "#feat_imp = pd.Series(bst.get_fscore().values(),index=column_name).sort_values(ascending=False)  \n",
    "feat_imp = pd.Series(bst.get_fscore().values()).sort_values(ascending=False)  \n",
    "feat_imp.plot(kind='bar', title='Feature Importances')  \n",
    "plt.ylabel('Feature Importance Score')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_prob = bst.predict(xg_test).reshape(test_Y.shape[0], 2)\n",
    "print pred_prob\n",
    "pred_label = np.argmax(pred_prob, axis=1)\n",
    "error_rate = np.sum(pred_label != test_Y) / test_Y.shape[0]\n",
    "print pred_label[0:20], test_Y[0:20] ,error_rate\n",
    "print('Test error using softprob = {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the predict probility for each event\n",
    "#f=open(\"xgb_predict.txt\",\"wb\")\n",
    "#f1=open(\"xgb_input.txt\",\"wb\")\n",
    "#pr=np.array(bst.predict(xg_val).reshape(val_Y.shape[0], 2)[:,1])\n",
    "#np.savetxt(f,pr)\n",
    "#np.savetxt(f,val_Y)\n",
    "#np.savetxt(f1,val_X[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pr_test=np.array(bst.predict(xg_test).reshape(test_Y.shape[0], 2))\n",
    "pr_phsp=np.array(bst.predict(xg_phsp_only).reshape(phsp_only_Y.shape[0], 2))\n",
    "pr_mctruth=np.array(bst.predict(xg_mctruth_only).reshape(mctruth_only_Y.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_test=pr_test[:,1]/pr_test[:,0]\n",
    "weight_phsp=pr_phsp[:,1]/pr_phsp[:,0]\n",
    "weight_mctruth=pr_mctruth[:,1]/pr_mctruth[:,0]\n",
    "\n",
    "#save the weight for each event\n",
    "f=open(\"xgb_weight.txt\",\"wb\")\n",
    "np.savetxt(f,weight_phsp)\n",
    "\n",
    "print 'The efficiency is = ',weight_phsp.sum()/weight_mctruth.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_DIY_truth_all=np.array(bst.predict(xg_DIY_truth_all).reshape(xg_DIY_truth_all_Y.shape[0], 2))\n",
    "weight_DIY_truth_all=pr_DIY_truth_all[:,1]/pr_DIY_truth_all[:,0]\n",
    "f=open(\"xgb_weight_DIY_truth_all.txt\",\"wb\")\n",
    "np.savetxt(f,weight_DIY_truth_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_distributions(phsp_ori, data_ori,weight_phsp,'phsp_target_xgboost.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_distributions(phsp_ori, data_ori,original_weights,'phsp_target_xgboost.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_distributions(mctruth_ori, data_ori,mctruth_weights,'phsp_target_ori.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original distributions\n",
    "KS = Kolmogorov-Smirnov distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hep_ml import reweight\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#data_gbdt = numpy.concatenate([original_train, target_train])\n",
    "#labels_gbdt = numpy.array([0] * len(original_train) + [1] * len(target_train))\n",
    "\n",
    "\n",
    "Xtr_gbdt, Xts_gbdt, Ytr_gbdt, Yts_gbdt = train_test_split(data_all, labels_all, random_state=42, train_size=0.8)\n",
    "clf_gbdt = GradientBoostingClassifier().fit(Xtr_gbdt, Ytr_gbdt)\n",
    "\n",
    "pred_label = np.argmax(clf_gbdt.predict_proba(Xts_gbdt), axis=1)\n",
    "error_rate_val = np.sum(pred_label != Yts_gbdt) / Yts_gbdt.shape[0]\n",
    "print('error  = {}'.format(error_rate_val))\n",
    "    \n",
    "pr_phsp=np.array(clf_gbdt.predict_proba(phsp_only_X).reshape(phsp_only_Y.shape[0], 2))\n",
    "pr_mctruth=np.array(clf_gbdt.predict_proba(mctruth_only_X).reshape(mctruth_only_Y.shape[0], 2))\n",
    "weight_phsp_GBT=pr_phsp[:,1]/pr_phsp[:,0]\n",
    "weight_mctruth_GBT=pr_mctruth[:,1]/pr_mctruth[:,0]\n",
    "print \"Efficient = \", weight_phsp_GBT.sum()/weight_mctruth_GBT.sum()\n",
    "\n",
    "draw_distributions(phsp_ori, data_ori,weight_phsp_GBT,'phsp_target_GBT.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hep_ml import reweight\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "#for paper\n",
    "#gb_weights_test = reweighter.predict_weights(original_test)\n",
    "\n",
    "    \n",
    "data = data_all\n",
    "labels = labels_all\n",
    "\n",
    "#get the weight factor for xgb_model\n",
    "xgb_X = np.array(phsp_train)\n",
    "xgb_Y = numpy.array([0] * len(xgb_X))\n",
    "xg_phsp = xgb.DMatrix(xgb_X, label=xgb_Y)\n",
    "pr_phsp_xgb=np.array(bst.predict(xg_phsp))\n",
    "weight_xgb=pr_phsp_xgb[:,1]/pr_phsp_xgb[:,0]\n",
    "\n",
    "#for sklearn gbdt\n",
    "pr_phsp_gbdt=np.array(clf_gbdt.predict_proba(phsp_train).reshape(phsp_train.shape[0], 2))\n",
    "#pr_phsp_gbdt=np.array(clf_gbdt.predict_proba(phsp_train))\n",
    "gbdt_weights_test=pr_phsp_gbdt[:,1]/pr_phsp_gbdt[:,0]\n",
    "draw_distributions(phsp_train, data_ori,gbdt_weights_test,'test.pdf')\n",
    "\n",
    "\n",
    "#new_weights=original_weights_test\n",
    "#W = numpy.concatenate([new_weights / new_weights.sum() * len(phsp_only_X), [1] * len(data_only_X)])\n",
    "W = numpy.concatenate([[1] * len(data_train),[1]*len(phsp_train)])\n",
    "Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.8)\n",
    "    \n",
    "clf_ori = GradientBoostingClassifier(subsample=0.3, n_estimators=50).fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "fpr_ori, tpr_ori, thresholds_ori = metrics.roc_curve(Yts,clf_ori.predict_proba(Xts)[:, 1])\n",
    "\n",
    "new_weights=weight_xgb\n",
    "W = numpy.concatenate([[1] * len(data_train),new_weights / new_weights.sum() * len(phsp_train)])\n",
    "Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.8)\n",
    "clf_xgb_t = GradientBoostingClassifier(subsample=0.3, n_estimators=50).fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = metrics.roc_curve(Yts,clf_xgb_t.predict_proba(Xts)[:, 1])\n",
    "\n",
    "new_weights=gbdt_weights_test\n",
    "W = numpy.concatenate([ [1] * len(data_train),new_weights / new_weights.sum() * len(phsp_train)])\n",
    "Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.8)\n",
    "clf_gbdt_t = GradientBoostingClassifier(subsample=0.3, n_estimators=50).fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "fpr_gbdt, tpr_gbdt, thresholds_gbdt = metrics.roc_curve(Yts,clf_gbdt_t.predict_proba(Xts)[:, 1])\n",
    "\n",
    "\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(Yts,Yts)\n",
    "plt.figure().set_size_inches(10.5, 9.5)\n",
    "#plt.plot(fpr_xgb,tpr_xgb,lw=2, color='darkblue',label='xgboost (AUC = %0.2f)' % metrics.auc(fpr_xgb,tpr_xgb))\n",
    "plt.plot(fpr_ori,tpr_ori,lw=2, color='darkorange',label='original (AUC = %0.2f)' % metrics.auc(fpr_ori,tpr_ori))\n",
    "plt.plot(fpr_gbdt,tpr_gbdt,lw=2, color='darkred',label='Reuse GBDT (AUC = %0.2f)' % metrics.auc(fpr_gbdt,tpr_gbdt))\n",
    "plt.plot(fpr_xgb,tpr_xgb,lw=2, color='darkblue',label='XGOOST (AUC = %0.2f)' % metrics.auc(fpr_xgb,tpr_xgb))\n",
    "plt.plot([0, 1], [0, 1], color='navy',label='random guessing',lw=2,linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate',fontsize=20)\n",
    "plt.ylabel('True Positive Rate',fontsize=20)\n",
    "plt.title('ROC curve',fontsize=25)                                                                                                  \n",
    "plt.legend(loc=\"lower right\",fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hep_ml import reweight\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#from sklearn import metrics\n",
    "\n",
    "#for paper\n",
    "#gb_weights_test = reweighter.predict_weights(original_test)\n",
    "\n",
    "    \n",
    "data = data_all\n",
    "labels = labels_all\n",
    "\n",
    "#get the weight factor for xgb_model\n",
    "xgb_X = np.array(phsp_train)\n",
    "xgb_Y = numpy.array([0] * len(xgb_X))\n",
    "xg_phsp = xgb.DMatrix(xgb_X, label=xgb_Y)\n",
    "pr_phsp_xgb=np.array(bst.predict(xg_phsp))\n",
    "weight_xgb=pr_phsp_xgb[:,1]/pr_phsp_xgb[:,0]\n",
    "\n",
    "#for sklearn gbdt\n",
    "pr_phsp_gbdt=np.array(clf_gbdt.predict_proba(phsp_train).reshape(phsp_train.shape[0], 2))\n",
    "#pr_phsp_gbdt=np.array(clf_gbdt.predict_proba(phsp_train))\n",
    "gbdt_weights_test=pr_phsp_gbdt[:,1]/pr_phsp_gbdt[:,0]\n",
    "draw_distributions(phsp_train, data_ori,gbdt_weights_test,'test.pdf')\n",
    "\n",
    "\n",
    "#new_weights=original_weights_test\n",
    "#W = numpy.concatenate([new_weights / new_weights.sum() * len(phsp_only_X), [1] * len(data_only_X)])\n",
    "W = numpy.concatenate([[1] * len(data_train),[1]*len(phsp_train)])\n",
    "Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.8)\n",
    "    \n",
    "xgboost_ori = xgb.XGBClassifier().fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "fpr_ori, tpr_ori, thresholds_ori = metrics.roc_curve(Yts,xgboost_ori.predict_proba(Xts)[:, 1])\n",
    "\n",
    "new_weights=weight_xgb\n",
    "W = numpy.concatenate([[1] * len(data_train),new_weights / new_weights.sum() * len(phsp_train)])\n",
    "Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.8)\n",
    "xgboost_xgb_t = xgb.XGBClassifier().fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = metrics.roc_curve(Yts,xgboost_xgb_t.predict_proba(Xts)[:, 1])\n",
    "\n",
    "new_weights=gbdt_weights_test\n",
    "W = numpy.concatenate([ [1] * len(data_train),new_weights / new_weights.sum() * len(phsp_train)])\n",
    "Xtr, Xts, Ytr, Yts, Wtr, Wts = train_test_split(data, labels, W, random_state=42, train_size=0.8)\n",
    "xgboost_gbdt_t = xgb.XGBClassifier().fit(Xtr, Ytr, sample_weight=Wtr)\n",
    "fpr_gbdt, tpr_gbdt, thresholds_gbdt = metrics.roc_curve(Yts,xgboost_gbdt_t.predict_proba(Xts)[:, 1])\n",
    "\n",
    "\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(Yts,Yts)\n",
    "plt.figure().set_size_inches(10.5, 9.5)\n",
    "#plt.plot(fpr_xgb,tpr_xgb,lw=2, color='darkblue',label='xgboost (AUC = %0.2f)' % metrics.auc(fpr_xgb,tpr_xgb))\n",
    "plt.plot(fpr_ori,tpr_ori,lw=2, color='darkorange',label='original (AUC = %0.2f)' % metrics.auc(fpr_ori,tpr_ori))\n",
    "plt.plot(fpr_gbdt,tpr_gbdt,lw=2, color='darkred',label='GBDT (AUC = %0.2f)' % metrics.auc(fpr_gbdt,tpr_gbdt))\n",
    "plt.plot(fpr_xgb,tpr_xgb,lw=2, color='darkblue',label='Reuse XGOOST (AUC = %0.2f)' % metrics.auc(fpr_xgb,tpr_xgb))\n",
    "plt.plot([0, 1], [0, 1], color='navy',label='random guessing',lw=2,linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel('False Positive Rate',fontsize=20)\n",
    "plt.ylabel('True Positive Rate',fontsize=20)\n",
    "plt.title('ROC curve',fontsize=25)                                                                                                  \n",
    "plt.legend(loc=\"lower right\",fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.plot(data_ori['ppi_mass'],data_ori['pn_mass'],'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
